{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sobre o projeto","text":""},{"location":"#projeto-apache-spark-com-apache-iceberg-e-apache-delta-lake","title":"Projeto Apache Spark com Apache Iceberg e Apache Delta Lake","text":"<p>Projeto desenvolvido para demonstra\u00e7\u00e3o do Apache Spark Local (pyspark) gravando arquivos no formato Apache Iceberg e Apache Delta Lake, tamb\u00e9m de forma local.</p> <p>Necess\u00e1rio possuir Python na vers\u00e3o 3.12, e Java na vers\u00e3o 8. Recomendado possuir sistema Linux para execu\u00e7\u00e3o do projeto.</p> <p>Projeto Python inicializado com o Poetry.</p> <p>Comandos utilizados para setup do ambiente:</p> <pre><code>poetry init\npoetry add pyspark=3.4.2 delta-spark=2.4.0 jupyterlab\npoetry shell\njupyter-lab\n</code></pre> <p>Os exemplos de c\u00f3digo pyspark/python para instanciar o Spark, bem como criar e manipular uma tabela Apache Iceberg e Apache Delta Lake,  foram fetios com base nos arquivos <code>iceberg.ipynb</code> e <code>delta-lake.ipynb</code>, respectivamente, no seguinte reposit\u00f3rio:</p> <pre><code>git clone https://github.com/jp-bonetti/atividade-engenharia-de-dados-pyspark\n</code></pre> <p>A cada nova execu\u00e7\u00e3o dos notebooks, apague a pasta data.</p>"},{"location":"#diagrama-er","title":"Diagrama ER","text":""},{"location":"deltalake/","title":"Delta Lake","text":""},{"location":"deltalake/#importando-as-dependencias","title":"Importando as depend\u00eancias","text":"<p>Para realizar as opera\u00e7\u00f5es, precisamos importar as seguintes bibliotecas, incluindo os tipos espec\u00edficos para cria\u00e7\u00e3o das tabelas.</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n\nfrom delta import *\n</code></pre>"},{"location":"deltalake/#criacao-da-classe-spark","title":"Cria\u00e7\u00e3o da classe Spark","text":"<p>Ap\u00f3s importar, j\u00e1 podemos inicializar nossa classe Spark, que vai ser respons\u00e1vel por toda a manipula\u00e7\u00e3o dos dados.</p> <pre><code>spark = ( \n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate() \n)\n</code></pre>"},{"location":"deltalake/#criacao-da-base-de-dados","title":"Cria\u00e7\u00e3o da base de dados","text":"<p>Nessa etapa, em data estamos criando a inje\u00e7\u00e3o inicial de dados para facilizar a manipula\u00e7\u00e3o futuramente.</p> <p>Em schema, declaramos a estrutura que vai compor o nosso projeto.</p> <pre><code>data = [\n    (\"1\", \"PRODUTO_X\",\"MARCA_X\", \"CATEGORIA_X\" ,800.00, 50),\n    (\"2\", \"PRODUTO_Y\", \"MARCA_Y\", \"CATEGORIA_Y\", 500.00, 100),\n    (\"3\", \"PRODUTO_Z\",\"MARCA_Z\", \"CATEGORIA_Z\", 150.00, 150)\n]\n\nschema = (\n    StructType([\n        StructField(\"ID_PRODUTO\", StringType(),True),\n        StructField(\"NOME_PRODUTO\", StringType(),True),\n        StructField(\"MARCA_PRODUTO\", StringType(),True),\n        StructField(\"CATEGORIA_PRODUTO\", StringType(),True),\n        StructField(\"PRECO_PRODUTO\", FloatType(), True),\n        StructField(\"QUANTIDADE_PRODUTO\", IntegerType(), True)\n    ])\n)\n\ndf = spark.createDataFrame(data=data,schema=schema)\n\ndf.show(truncate=False)\n</code></pre>"},{"location":"deltalake/#salvando-o-dataframe-no-formato-delta","title":"Salvando o DataFrame no formato Delta","text":"<p>Ap\u00f3s definir a estrutura e os dados inicias, vamos salvar o DataFrame no formato Delta, configurar para que os dados sejam sobrescritos, e informar onde os dados devem ser salvos.</p> <pre><code>( \n    df\n    .write\n    .format(\"delta\")\n    .mode('overwrite')\n    .save(\"./data/PRODUTOS\")\n)\n</code></pre>"},{"location":"deltalake/#insert","title":"INSERT","text":"<p>Defini\u00e7\u00e3o dos novos dados a serem inseridos.</p> <pre><code>new_data = [\n    (\"4\",\"PRODUTO_A\", \"MARCA_A\", \"CATEGORIA_A\", 50.00, 60),\n    (\"5\",\"PRODUTO_B\", \"MARCA_B\", \"CATEGORIA_B\", 150.00, 90),\n    (\"6\",\"PRODUTO_C\", \"MARCA_C\", \"CATEGORIA_C\", 300.00, 20)\n]\n\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\ndeltaTable = DeltaTable.forPath(spark, \"./data/PRODUTOS\")\n</code></pre> <p>Opera\u00e7\u00e3o de marge entra o DataFrame j\u00e1 existente, e o criado acima.</p> <pre><code>(\n    deltaTable.alias(\"dados_atuais\")\n    .merge(\n        df_new.alias(\"novos_dados\"),\n        \"dados_atuais.ID_PRODUTO = novos_dados.ID_PRODUTO\"\n    )\n    .whenMatchedUpdateAll()\n    .whenNotMatchedInsertAll()\n    .execute()\n)\n</code></pre>"},{"location":"deltalake/#delete","title":"DELETE","text":"<p>Opera\u00e7\u00e3o de deletar com base no ID do produto.</p> <pre><code>deltaTable.delete(\"ID_PRODUTO = 4\")\n</code></pre>"},{"location":"deltalake/#update","title":"UPDATE","text":"<p>Opera\u00e7\u00e3o de atualizar o pre\u00e7o do produto com base no seu ID.</p> <pre><code>deltaTable.update(\"ID_PRODUTO = 2\", set = { \"PRECO_PRODUTO\": \"400.00\" })\n</code></pre>"},{"location":"deltalake/#exibir-o-estado-atual-da-tabela","title":"Exibir o estado atual da tabela","text":"<pre><code>(\n    spark\n    .read\n    .format('delta')\n    .load(\"./data/PRODUTOS\")\n    .show()\n)\n</code></pre>"},{"location":"deltalake/#historico-de-alteracoes","title":"Hist\u00f3rico de altera\u00e7\u00f5es","text":"<pre><code>(\n    deltaTable\n    .history()\n    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n    .show()\n)\n</code></pre>"},{"location":"iceberg/","title":"Iceberg","text":""},{"location":"iceberg/#importando-as-dependencias","title":"Importando as depend\u00eancias","text":"<p>Para realizar as opera\u00e7\u00f5es, precisamos importar a seguinte biblioteca.</p> <pre><code>from pyspark.sql import SparkSession\n</code></pre>"},{"location":"iceberg/#criacao-da-classe-spark","title":"Cria\u00e7\u00e3o da classe Spark","text":"<p>Ap\u00f3s importar, j\u00e1 podemos inicializar nossa classe Spark, que vai ser respons\u00e1vel por toda a manipula\u00e7\u00e3o dos nossos dados.</p> <pre><code>spark = SparkSession.builder \\\n  .appName(\"IcebergLocalDevelopment\") \\\n  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0') \\\n  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n  .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n  .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n  .config(\"spark.sql.catalog.local.warehouse\", \"data/spark-warehouse/iceberg\") \\\n  .getOrCreate()\n</code></pre> <p>Conforme ser\u00e1 mostrado, toda a manipula\u00e7\u00e3o \u00e9 feita atrav\u00e9s de comandos SQL.</p>"},{"location":"iceberg/#criacao-da-base-de-dados","title":"Cria\u00e7\u00e3o da base de dados","text":"<pre><code>spark.sql(\n  \"\"\"\n  CREATE TABLE local.PRODUTOS (ID_PRODUTO INT, NOME_PRODUTO STRING, MARCA_PRODUTO STRING, CATEGORIA_PRODUTO STRING, PRECO_PRODUTO DOUBLE, QUANTIDADE_PRODUTO INT) USING iceberg\n  \"\"\"\n)\n</code></pre>"},{"location":"iceberg/#insert","title":"INSERT","text":"<pre><code>spark.sql('''INSERT INTO local.PRODUTOS VALUES \n    (1, \"PRODUTO_X\",\"MARCA_X\", \"CATEGORIA_X\" ,800.00, 50),\n    (2, \"PRODUTO_Y\", \"MARCA_Y\", \"CATEGORIA_Y\", 500.00, 100),\n    (3, \"PRODUTO_Z\",\"MARCA_Z\", \"CATEGORIA_Z\", 150.00, 150)''')\n</code></pre>"},{"location":"iceberg/#delete","title":"DELETE","text":"<p>Opera\u00e7\u00e3o de deletar com base no ID do produto.</p> <pre><code>spark.sql('delete from local.PRODUTOS where ID_PRODUTO = 3')\n</code></pre>"},{"location":"iceberg/#update","title":"UPDATE","text":"<p>Opera\u00e7\u00e3o de atualizar o pre\u00e7o do produto com base no seu ID.</p> <pre><code>spark.sql(\n    \"\"\"\n    update local.PRODUTOS set ANO_FABRICACAO_PRODUTO = '2023' where ID_PRODUTO = 3\n    \"\"\"\n)\n</code></pre>"},{"location":"iceberg/#select","title":"SELECT","text":"<pre><code>spark.sql('select * from local.PRODUTOS').show()\n</code></pre>"}]}